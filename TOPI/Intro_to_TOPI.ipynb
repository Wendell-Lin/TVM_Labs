{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "028db66a",
   "metadata": {},
   "source": [
    "# Intro\n",
    "TVM OPerator Invetory(TOPI), is a numpy-style generic op & sch w/ higher abstraction in TVM\n",
    "- Basic usage\n",
    "- Numpy-style TOPI\n",
    "- Operator Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f30d90b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "import tvm.testing\n",
    "from tvm import te\n",
    "from tvm import topi\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3891053b",
   "metadata": {},
   "source": [
    "## Basic\n",
    "Recover TE by base case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78636035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*n: int32)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [n, m: int32], [stride, stride_1: int32], type=\"auto\")} {\n",
      "  allocate(B: Pointer(global float32), float32, [n]), storage_scope = global;\n",
      "  for (i: int32, 0, n) {\n",
      "    B_1: Buffer(B, float32, [n], [])[i] = 0f32\n",
      "    for (k: int32, 0, m) {\n",
      "      B_1[i] = (B_1[i] + A[((i*stride) + (k*stride_1))])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = te.var(\"n\")\n",
    "m = te.var(\"m\")\n",
    "A = te.placeholder((n, m), name=\"A\")\n",
    "k = te.reduce_axis((0, m), \"k\")\n",
    "B = te.compute((n,), lambda i: te.sum(A[i, k], axis=k), name=\"B\")\n",
    "s = te.create_schedule(B.op)\n",
    "print(tvm.lower(s, [A], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e8ce7",
   "metadata": {},
   "source": [
    "TE still need `te.reduce_axis` & `te.compute` to get sum, but TOPI only need `topi.sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d22f1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [(stride: int32*n: int32)], [], type=\"auto\")}\n",
      "  buffer_map = {A_1: A}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [n, m: int32], [stride, stride_1: int32], type=\"auto\")} {\n",
      "  allocate(A_red: Pointer(global float32), float32, [n]), storage_scope = global;\n",
      "  for (ax0: int32, 0, n) {\n",
      "    A_red_1: Buffer(A_red, float32, [n], [])[ax0] = 0f32\n",
      "    for (k1: int32, 0, m) {\n",
      "      A_red_1[ax0] = (A_red_1[ax0] + A[((ax0*stride) + (k1*stride_1))])\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C = topi.sum(A, axis=1)\n",
    "ts = te.create_schedule(C.op)\n",
    "print(tvm.lower(ts, [A], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557e573",
   "metadata": {},
   "source": [
    "## Numpy-style op broadcasting by overloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9219441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(a_1: handle, b_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {a: Buffer(a_2: Pointer(float32), float32, [10000], []),\n",
      "             b: Buffer(b_2: Pointer(float32), float32, [100], [])}\n",
      "  buffer_map = {a_1: a, b_1: b}\n",
      "  preflattened_buffer_map = {a_1: a_3: Buffer(a_2, float32, [100, 10, 10], []), b_1: b_3: Buffer(b_2, float32, [10, 10], [])} {\n",
      "  allocate(T_divide_red: Pointer(global float32), float32, [1]), storage_scope = global;\n",
      "  attr [IterVar(threadIdx.x: int32, [0:1024], \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 1024;\n",
      "  allocate(T_divide_red.rf: Pointer(local float32), float32, [1]), storage_scope = local;\n",
      "  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local {\n",
      "    T_divide_red.rf_1: Buffer(T_divide_red.rf, float32, [1], [], scope=\"local\", align=4)[0] = 0f32\n",
      "    for (k0.k1.fused.k2.fused.outer: int32, 0, 10) {\n",
      "      if @tir.likely((((((k0.k1.fused.k2.fused.outer*64) + floordiv(threadIdx.x, 16)) < 625) && (((k0.k1.fused.k2.fused.outer*64) + floordiv(threadIdx.x, 16)) < 625)) && (((k0.k1.fused.k2.fused.outer*64) + floordiv(threadIdx.x, 16)) < 625)), dtype=bool) {\n",
      "        T_divide_red.rf_1[0] = (T_divide_red.rf_1[0] + (((a[((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x)] + b[((floordiv(floormod(((k0.k1.fused.k2.fused.outer*12) + floordiv(threadIdx.x, 2)), 50), 5)*10) + floormod(((k0.k1.fused.k2.fused.outer*4) + threadIdx.x), 10))]) + (a[((k0.k1.fused.k2.fused.outer*1024) + threadIdx.x)]*b[((floordiv(floormod(((k0.k1.fused.k2.fused.outer*12) + floordiv(threadIdx.x, 2)), 50), 5)*10) + floormod(((k0.k1.fused.k2.fused.outer*4) + threadIdx.x), 10))]))*0.5f32))\n",
      "      }\n",
      "    }\n",
      "    attr [meta[tir.CommReducer][0]] \"reduce_scope\" = @tir.reinterpret(0u64, dtype=handle);\n",
      "    @tir.tvm_thread_allreduce(1u32, T_divide_red.rf_1[0], True, reduce_temp0_1: Buffer(reduce_temp0, float32, [1], [], scope=\"local\")[0], threadIdx.x, dtype=handle)\n",
      "    if (threadIdx.x == 0) {\n",
      "      T_divide_red_1: Buffer(T_divide_red, float32, [1], [], align=4)[0] = reduce_temp0_1[0]\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, y = 100, 10\n",
    "a = te.placeholder((x, y, y), name='a')\n",
    "b = te.placeholder((y, y), name='b')\n",
    "c = a + b # np style, same as topi.broadcast_add\n",
    "d = a * b # np style, same as topi.broadcast_mul\n",
    "e = topi.elemwise_sum([c, d])\n",
    "f = e / 2. # np style, also primitive\n",
    "g = topi.sum(f)\n",
    "with tvm.target.cuda(model=\"3090\", arch=\"sm_86\"):\n",
    "    sg = topi.cuda.schedule_reduce(g)\n",
    "    print(tvm.lower(sg, [a, b], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e995e69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stage(a, placeholder(a, 0x3d45ad0)), stage(b, placeholder(b, 0x3270840)), stage(T_add, compute(T_add, body=[(a[ax0, ax1, ax2] + b[ax1, ax2])], axis=[iter_var(ax0, range(min=0, ext=100)), iter_var(ax1, range(min=0, ext=10)), iter_var(ax2, range(min=0, ext=10))], reduce_axis=[], tag=broadcast, attrs={})), stage(T_multiply, compute(T_multiply, body=[(a[ax0, ax1, ax2]*b[ax1, ax2])], axis=[iter_var(ax0, range(min=0, ext=100)), iter_var(ax1, range(min=0, ext=10)), iter_var(ax2, range(min=0, ext=10))], reduce_axis=[], tag=broadcast, attrs={})), stage(T_elemwise_sum, compute(T_elemwise_sum, body=[(T_add[ax0, ax1, ax2] + T_multiply[ax0, ax1, ax2])], axis=[iter_var(ax0, range(min=0, ext=100)), iter_var(ax1, range(min=0, ext=10)), iter_var(ax2, range(min=0, ext=10))], reduce_axis=[], tag=elemwise, attrs={})), stage(T_divide, compute(T_divide, body=[(T_elemwise_sum[ax0, ax1, ax2]/2f)], axis=[iter_var(ax0, range(min=0, ext=100)), iter_var(ax1, range(min=0, ext=10)), iter_var(ax2, range(min=0, ext=10))], reduce_axis=[], tag=elemwise, attrs={})), stage(T_divide_red.rf, compute(T_divide_red.rf, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_divide[floordiv(floordiv((k0.k1.fused.k2.fused.inner + (k0.k1.fused.k2.fused.outer*1024)), 10), 10), floormod(floordiv((k0.k1.fused.k2.fused.inner + (k0.k1.fused.k2.fused.outer*1024)), 10), 10), floormod((k0.k1.fused.k2.fused.inner + (k0.k1.fused.k2.fused.outer*1024)), 10)]], init=[], axis=[iter_var(k0.k1.fused.k2.fused.outer, range(min=0, ext=10))], where=tir.likely((((floordiv(floordiv((k0.k1.fused.k2.fused.inner + (k0.k1.fused.k2.fused.outer*1024)), 10), 10) < 100) && (floordiv((k0.k1.fused.k2.fused.inner + (k0.k1.fused.k2.fused.outer*1024)), 10) < 1000)) && ((k0.k1.fused.k2.fused.inner + (k0.k1.fused.k2.fused.outer*1024)) < 10000))), value_index=0)], axis=[iter_var(k0.k1.fused.k2.fused.inner, range(min=0, ext=1024))], reduce_axis=[iter_var(k0.k1.fused.k2.fused.outer, range(min=0, ext=10))], tag=, attrs={})), stage(T_divide_red, compute(T_divide_red.repl, body=[reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[T_divide_red.rf[k0.k1.fused.k2.fused.inner.v]], init=[], axis=[iter_var(k0.k1.fused.k2.fused.inner.v, range(min=0, ext=1024))], where=(bool)1, value_index=0)], axis=[], reduce_axis=[iter_var(k0.k1.fused.k2.fused.inner.v, range(min=0, ext=1024))], tag=, attrs={}))]\n"
     ]
    }
   ],
   "source": [
    "print(sg.stages) # to examine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e7d1d3",
   "metadata": {},
   "source": [
    "## Test w/ Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29cc9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = tvm.build(sg, [a, b, g], \"cuda -arch=sm_86\")\n",
    "dev = tvm.cuda(0)\n",
    "a_np = np.random.uniform(size=(x, y, y)).astype(a.dtype)\n",
    "b_np = np.random.uniform(size=(y, y)).astype(b.dtype)\n",
    "g_np = np.sum(np.add(a_np + b_np, a_np * b_np) / 2.0)\n",
    "a_nd = tvm.nd.array(a_np, dev)\n",
    "b_nd = tvm.nd.array(b_np, dev)\n",
    "g_nd = tvm.nd.array(np.zeros(g_np.shape, dtype=g_np.dtype), dev)\n",
    "func(a_nd, b_nd, g_nd)\n",
    "tvm.testing.assert_allclose(g_nd.numpy(), g_np, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f591eed3",
   "metadata": {},
   "source": [
    "## Common NN OP in NUMPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4245e90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(tarray_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {tarray: Buffer(tarray_2: Pointer(float32), float32, [262144], [])}\n",
      "  buffer_map = {tarray_1: tarray}\n",
      "  preflattened_buffer_map = {tarray_1: tarray_3: Buffer(tarray_2, float32, [512, 512], [])} {\n",
      "  allocate(T_softmax_norm: Pointer(global float32x4), float32x4, [65536]), storage_scope = global;\n",
      "  attr [IterVar(blockIdx.x: int32, (nullptr), \"ThreadIndex\", \"blockIdx.x\")] \"thread_extent\" = 512;\n",
      "  allocate(normal_reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;\n",
      "  allocate(reduce_temp0: Pointer(local float32), float32, [1]), storage_scope = local;\n",
      "  allocate(T_softmax_exp: Pointer(warp float32), float32, [512]), storage_scope = warp;\n",
      "  allocate(normal_reduce_temp0_1: Pointer(local float32), float32, [1]), storage_scope = local;\n",
      "  allocate(reduce_temp0_1: Pointer(local float32), float32, [1]), storage_scope = local {\n",
      "    attr [IterVar(threadIdx.x: int32, [0:32], \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 32 {\n",
      "      normal_reduce_temp0_2: Buffer(normal_reduce_temp0, float32, [1], [], scope=\"local\")[0] = -3.40282e+38f32\n",
      "      for (k.inner: int32, 0, 16) {\n",
      "        normal_reduce_temp0_2[0] = max(normal_reduce_temp0_2[0], tarray[(((blockIdx.x*512) + (threadIdx.x*16)) + k.inner)])\n",
      "      }\n",
      "      attr [meta[tir.CommReducer][0]] \"reduce_scope\" = @tir.reinterpret(0u64, dtype=handle);\n",
      "      @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_2[0], True, reduce_temp0_2: Buffer(reduce_temp0, float32, [1], [], scope=\"local\")[0], threadIdx.x, dtype=handle)\n",
      "      for (i1.inner.outer: int32, 0, 4) {\n",
      "        let cse_var_1: int32 = (i1.inner.outer*4)\n",
      "        T_softmax_exp_1: Buffer(T_softmax_exp, float32, [512], [], scope=\"warp\")[ramp(((threadIdx.x*16) + cse_var_1), 1, 4)] = @tir.exp((tarray[ramp((((blockIdx.x*512) + (threadIdx.x*16)) + cse_var_1), 1, 4)] - broadcast(reduce_temp0_3: Buffer(reduce_temp0, float32, [1], [], scope=\"local\", align=4)[0], 4)), dtype=float32x4)\n",
      "      }\n",
      "    }\n",
      "    attr [IterVar(threadIdx.x, [0:32], \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 32 {\n",
      "      normal_reduce_temp0_3: Buffer(normal_reduce_temp0_1, float32, [1], [], scope=\"local\")[0] = 0f32\n",
      "      for (k.inner_1: int32, 0, 16) {\n",
      "        normal_reduce_temp0_3[0] = (normal_reduce_temp0_3[0] + T_softmax_exp_1[((threadIdx.x*16) + k.inner_1)])\n",
      "      }\n",
      "      attr [meta[tir.CommReducer][1]] \"reduce_scope\" = @tir.reinterpret(0u64, dtype=handle);\n",
      "      @tir.tvm_thread_allreduce(1u32, normal_reduce_temp0_3[0], True, reduce_temp0_4: Buffer(reduce_temp0_1, float32, [1], [], scope=\"local\")[0], threadIdx.x, dtype=handle)\n",
      "      for (i1.inner.outer_1: int32, 0, 4) {\n",
      "        T_softmax_norm_1: Buffer(T_softmax_norm, float32x4, [65536], [])[(((blockIdx.x*128) + (threadIdx.x*4)) + i1.inner.outer_1)] = (T_softmax_exp_1[ramp(((threadIdx.x*16) + (i1.inner.outer_1*4)), 1, 4)] / broadcast(reduce_temp0_5: Buffer(reduce_temp0_1, float32, [1], [], scope=\"local\", align=4)[0], 4))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tarray = te.placeholder((512, 512), name=\"tarray\")\n",
    "softmax_topi = topi.nn.softmax(tarray)\n",
    "with tvm.target.Target(\"cuda -arch=sm_86\"):\n",
    "    sst = topi.cuda.schedule_softmax(softmax_topi)\n",
    "    print(tvm.lower(sst, [tarray], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5786a2e2",
   "metadata": {},
   "source": [
    "## FuseOps, eg. conv2d + relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f82b351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(placeholder_2: handle, placeholder_3: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {placeholder: Buffer(placeholder_4: Pointer(float32), float32, [150528], []),\n",
      "             placeholder_1: Buffer(placeholder_5: Pointer(float32), float32, [750], [])}\n",
      "  buffer_map = {placeholder_2: placeholder, placeholder_3: placeholder_1}\n",
      "  preflattened_buffer_map = {placeholder_2: placeholder_6: Buffer(placeholder_4, float32, [1, 3, 224, 224], []), placeholder_3: placeholder_7: Buffer(placeholder_5, float32, [10, 3, 5, 5], [])} {\n",
      "  allocate(compute: Pointer(global float32), float32, [501760]), storage_scope = global;\n",
      "  attr [IterVar(blockIdx.z: int32, (nullptr), \"ThreadIndex\", \"blockIdx.z\")] \"thread_extent\" = 5;\n",
      "  allocate(conv2d_nchw: Pointer(local float32), float32, [14]), storage_scope = local;\n",
      "  allocate(pad_temp.shared: Pointer(shared float32), float32, [112]), storage_scope = shared;\n",
      "  allocate(placeholder.shared: Pointer(shared float32), float32, [2]), storage_scope = shared;\n",
      "  attr [IterVar(blockIdx.y: int32, (nullptr), \"ThreadIndex\", \"blockIdx.y\")] \"thread_extent\" = 224;\n",
      "  attr [IterVar(blockIdx.x: int32, (nullptr), \"ThreadIndex\", \"blockIdx.x\")] \"thread_extent\" = 2;\n",
      "  attr [IterVar(threadIdx.z: int32, (nullptr), \"ThreadIndex\", \"threadIdx.z\")] \"thread_extent\" = 1;\n",
      "  attr [IterVar(threadIdx.y: int32, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 1;\n",
      "  attr [IterVar(threadIdx.x: int32, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 16 {\n",
      "    conv2d_nchw_1: Buffer(conv2d_nchw, float32, [4], [], scope=\"local\", align=8)[0] = 0f32\n",
      "    conv2d_nchw_1[2] = 0f32\n",
      "    conv2d_nchw_1[4] = 0f32\n",
      "    conv2d_nchw_1[6] = 0f32\n",
      "    conv2d_nchw_1[8] = 0f32\n",
      "    conv2d_nchw_1[10] = 0f32\n",
      "    conv2d_nchw_1[12] = 0f32\n",
      "    conv2d_nchw_1[1] = 0f32\n",
      "    conv2d_nchw_1[3] = 0f32\n",
      "    conv2d_nchw_1[5] = 0f32\n",
      "    conv2d_nchw_1[7] = 0f32\n",
      "    conv2d_nchw_1[9] = 0f32\n",
      "    conv2d_nchw_1[11] = 0f32\n",
      "    conv2d_nchw_1[13] = 0f32\n",
      "    for (rc.outer: int32, 0, 3) {\n",
      "      for (ry.outer: int32, 0, 5) {\n",
      "        attr [IterVar(threadIdx.z_1: int32, (nullptr), \"ThreadIndex\", \"threadIdx.z\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.y_1: int32, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.x_1: int32, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 16 {\n",
      "          pad_temp.shared_1: Buffer(pad_temp.shared, float32, [112], [], scope=\"shared\")[(threadIdx.x_1*7)] = @tir.if_then_else((((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)) && (1 <= ((blockIdx.x*56) + floordiv((threadIdx.x_1*7), 2)))), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 450)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 1)] = @tir.if_then_else((((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)) && (1 <= ((blockIdx.x*56) + floordiv(((threadIdx.x_1*7) + 1), 2)))), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 449)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 2)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 448)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 3)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 447)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 4)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 446)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 5)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 445)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 6)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 444)], 0f32, dtype=float32)\n",
      "        }\n",
      "        attr [IterVar(threadIdx.z_2: int32, (nullptr), \"ThreadIndex\", \"threadIdx.z\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.y_2: int32, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.x_2: int32, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 16;\n",
      "        if @tir.likely((threadIdx.x_2 < 2), dtype=bool) {\n",
      "          placeholder.shared_1: Buffer(placeholder.shared, float32, [2], [], scope=\"shared\", align=8)[threadIdx.x_2] = placeholder_1[((((blockIdx.z*150) + (threadIdx.x_2*75)) + (rc.outer*25)) + (ry.outer*5))]\n",
      "        }\n",
      "        conv2d_nchw_1[0] = (conv2d_nchw_1[0] + (pad_temp.shared_1[threadIdx.x]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[2] = (conv2d_nchw_1[2] + (pad_temp.shared_1[(threadIdx.x + 16)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[4] = (conv2d_nchw_1[4] + (pad_temp.shared_1[(threadIdx.x + 32)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[6] = (conv2d_nchw_1[6] + (pad_temp.shared_1[(threadIdx.x + 48)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[8] = (conv2d_nchw_1[8] + (pad_temp.shared_1[(threadIdx.x + 64)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[10] = (conv2d_nchw_1[10] + (pad_temp.shared_1[(threadIdx.x + 80)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[12] = (conv2d_nchw_1[12] + (pad_temp.shared_1[(threadIdx.x + 96)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[1] = (conv2d_nchw_1[1] + (pad_temp.shared_1[threadIdx.x]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[3] = (conv2d_nchw_1[3] + (pad_temp.shared_1[(threadIdx.x + 16)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[5] = (conv2d_nchw_1[5] + (pad_temp.shared_1[(threadIdx.x + 32)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[7] = (conv2d_nchw_1[7] + (pad_temp.shared_1[(threadIdx.x + 48)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[9] = (conv2d_nchw_1[9] + (pad_temp.shared_1[(threadIdx.x + 64)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[11] = (conv2d_nchw_1[11] + (pad_temp.shared_1[(threadIdx.x + 80)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[13] = (conv2d_nchw_1[13] + (pad_temp.shared_1[(threadIdx.x + 96)]*placeholder.shared_1[1]))\n",
      "        attr [IterVar(threadIdx.z_1, (nullptr), \"ThreadIndex\", \"threadIdx.z\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.y_1, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.x_1, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 16 {\n",
      "          pad_temp.shared_1[(threadIdx.x_1*7)] = @tir.if_then_else((((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)) && (1 <= ((blockIdx.x*56) + floordiv(((threadIdx.x_1*7) + 1), 2)))), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 449)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 1)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 448)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 2)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 447)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 3)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 446)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 4)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 445)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 5)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 444)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 6)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 443)], 0f32, dtype=float32)\n",
      "        }\n",
      "        attr [IterVar(threadIdx.z_2, (nullptr), \"ThreadIndex\", \"threadIdx.z\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.y_2, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.x_2, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 16;\n",
      "        if @tir.likely((threadIdx.x_2 < 2), dtype=bool) {\n",
      "          placeholder.shared_1[threadIdx.x_2] = placeholder_1[(((((blockIdx.z*150) + (threadIdx.x_2*75)) + (rc.outer*25)) + (ry.outer*5)) + 1)]\n",
      "        }\n",
      "        conv2d_nchw_1[0] = (conv2d_nchw_1[0] + (pad_temp.shared_1[threadIdx.x]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[2] = (conv2d_nchw_1[2] + (pad_temp.shared_1[(threadIdx.x + 16)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[4] = (conv2d_nchw_1[4] + (pad_temp.shared_1[(threadIdx.x + 32)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[6] = (conv2d_nchw_1[6] + (pad_temp.shared_1[(threadIdx.x + 48)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[8] = (conv2d_nchw_1[8] + (pad_temp.shared_1[(threadIdx.x + 64)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[10] = (conv2d_nchw_1[10] + (pad_temp.shared_1[(threadIdx.x + 80)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[12] = (conv2d_nchw_1[12] + (pad_temp.shared_1[(threadIdx.x + 96)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[1] = (conv2d_nchw_1[1] + (pad_temp.shared_1[threadIdx.x]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[3] = (conv2d_nchw_1[3] + (pad_temp.shared_1[(threadIdx.x + 16)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[5] = (conv2d_nchw_1[5] + (pad_temp.shared_1[(threadIdx.x + 32)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[7] = (conv2d_nchw_1[7] + (pad_temp.shared_1[(threadIdx.x + 48)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[9] = (conv2d_nchw_1[9] + (pad_temp.shared_1[(threadIdx.x + 64)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[11] = (conv2d_nchw_1[11] + (pad_temp.shared_1[(threadIdx.x + 80)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[13] = (conv2d_nchw_1[13] + (pad_temp.shared_1[(threadIdx.x + 96)]*placeholder.shared_1[1]))\n",
      "        attr [IterVar(threadIdx.z_1, (nullptr), \"ThreadIndex\", \"threadIdx.z\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.y_1, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.x_1, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 16 {\n",
      "          pad_temp.shared_1[(threadIdx.x_1*7)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 448)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 1)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 447)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 2)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 446)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 3)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 445)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 4)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 444)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 5)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 443)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 6)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 442)], 0f32, dtype=float32)\n",
      "        }\n",
      "        attr [IterVar(threadIdx.z_2, (nullptr), \"ThreadIndex\", \"threadIdx.z\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.y_2, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.x_2, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 16;\n",
      "        if @tir.likely((threadIdx.x_2 < 2), dtype=bool) {\n",
      "          placeholder.shared_1[threadIdx.x_2] = placeholder_1[(((((blockIdx.z*150) + (threadIdx.x_2*75)) + (rc.outer*25)) + (ry.outer*5)) + 2)]\n",
      "        }\n",
      "        conv2d_nchw_1[0] = (conv2d_nchw_1[0] + (pad_temp.shared_1[threadIdx.x]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[2] = (conv2d_nchw_1[2] + (pad_temp.shared_1[(threadIdx.x + 16)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[4] = (conv2d_nchw_1[4] + (pad_temp.shared_1[(threadIdx.x + 32)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[6] = (conv2d_nchw_1[6] + (pad_temp.shared_1[(threadIdx.x + 48)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[8] = (conv2d_nchw_1[8] + (pad_temp.shared_1[(threadIdx.x + 64)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[10] = (conv2d_nchw_1[10] + (pad_temp.shared_1[(threadIdx.x + 80)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[12] = (conv2d_nchw_1[12] + (pad_temp.shared_1[(threadIdx.x + 96)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[1] = (conv2d_nchw_1[1] + (pad_temp.shared_1[threadIdx.x]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[3] = (conv2d_nchw_1[3] + (pad_temp.shared_1[(threadIdx.x + 16)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[5] = (conv2d_nchw_1[5] + (pad_temp.shared_1[(threadIdx.x + 32)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[7] = (conv2d_nchw_1[7] + (pad_temp.shared_1[(threadIdx.x + 48)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[9] = (conv2d_nchw_1[9] + (pad_temp.shared_1[(threadIdx.x + 64)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[11] = (conv2d_nchw_1[11] + (pad_temp.shared_1[(threadIdx.x + 80)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[13] = (conv2d_nchw_1[13] + (pad_temp.shared_1[(threadIdx.x + 96)]*placeholder.shared_1[1]))\n",
      "        attr [IterVar(threadIdx.z_1, (nullptr), \"ThreadIndex\", \"threadIdx.z\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.y_1, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.x_1, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 16 {\n",
      "          pad_temp.shared_1[(threadIdx.x_1*7)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 447)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 1)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 446)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 2)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 445)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 3)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 444)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 4)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 443)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 5)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 442)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 6)] = @tir.if_then_else((((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)) && (((blockIdx.x*56) + floordiv(((threadIdx.x_1*7) + 9), 2)) < 113)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 441)], 0f32, dtype=float32)\n",
      "        }\n",
      "        attr [IterVar(threadIdx.z_2, (nullptr), \"ThreadIndex\", \"threadIdx.z\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.y_2, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.x_2, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 16;\n",
      "        if @tir.likely((threadIdx.x_2 < 2), dtype=bool) {\n",
      "          placeholder.shared_1[threadIdx.x_2] = placeholder_1[(((((blockIdx.z*150) + (threadIdx.x_2*75)) + (rc.outer*25)) + (ry.outer*5)) + 3)]\n",
      "        }\n",
      "        conv2d_nchw_1[0] = (conv2d_nchw_1[0] + (pad_temp.shared_1[threadIdx.x]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[2] = (conv2d_nchw_1[2] + (pad_temp.shared_1[(threadIdx.x + 16)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[4] = (conv2d_nchw_1[4] + (pad_temp.shared_1[(threadIdx.x + 32)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[6] = (conv2d_nchw_1[6] + (pad_temp.shared_1[(threadIdx.x + 48)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[8] = (conv2d_nchw_1[8] + (pad_temp.shared_1[(threadIdx.x + 64)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[10] = (conv2d_nchw_1[10] + (pad_temp.shared_1[(threadIdx.x + 80)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[12] = (conv2d_nchw_1[12] + (pad_temp.shared_1[(threadIdx.x + 96)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[1] = (conv2d_nchw_1[1] + (pad_temp.shared_1[threadIdx.x]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[3] = (conv2d_nchw_1[3] + (pad_temp.shared_1[(threadIdx.x + 16)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[5] = (conv2d_nchw_1[5] + (pad_temp.shared_1[(threadIdx.x + 32)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[7] = (conv2d_nchw_1[7] + (pad_temp.shared_1[(threadIdx.x + 48)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[9] = (conv2d_nchw_1[9] + (pad_temp.shared_1[(threadIdx.x + 64)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[11] = (conv2d_nchw_1[11] + (pad_temp.shared_1[(threadIdx.x + 80)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[13] = (conv2d_nchw_1[13] + (pad_temp.shared_1[(threadIdx.x + 96)]*placeholder.shared_1[1]))\n",
      "        attr [IterVar(threadIdx.z_1, (nullptr), \"ThreadIndex\", \"threadIdx.z\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.y_1, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.x_1, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 16 {\n",
      "          pad_temp.shared_1[(threadIdx.x_1*7)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 446)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 1)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 445)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 2)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 444)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 3)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 443)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 4)] = @tir.if_then_else(((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 442)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 5)] = @tir.if_then_else((((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)) && (((blockIdx.x*56) + floordiv(((threadIdx.x_1*7) + 9), 2)) < 113)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 441)], 0f32, dtype=float32)\n",
      "          pad_temp.shared_1[((threadIdx.x_1*7) + 6)] = @tir.if_then_else((((2 <= (blockIdx.y + ry.outer)) && ((blockIdx.y + ry.outer) < 226)) && (((blockIdx.x*56) + floordiv((threadIdx.x_1*7), 2)) < 108)), placeholder[((((((rc.outer*50176) + (blockIdx.y*224)) + (ry.outer*224)) + (blockIdx.x*112)) + (threadIdx.x_1*7)) - 440)], 0f32, dtype=float32)\n",
      "        }\n",
      "        attr [IterVar(threadIdx.z_2, (nullptr), \"ThreadIndex\", \"threadIdx.z\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.y_2, (nullptr), \"ThreadIndex\", \"threadIdx.y\")] \"thread_extent\" = 1;\n",
      "        attr [IterVar(threadIdx.x_2, (nullptr), \"ThreadIndex\", \"threadIdx.x\")] \"thread_extent\" = 16;\n",
      "        if @tir.likely((threadIdx.x_2 < 2), dtype=bool) {\n",
      "          placeholder.shared_1[threadIdx.x_2] = placeholder_1[(((((blockIdx.z*150) + (threadIdx.x_2*75)) + (rc.outer*25)) + (ry.outer*5)) + 4)]\n",
      "        }\n",
      "        conv2d_nchw_1[0] = (conv2d_nchw_1[0] + (pad_temp.shared_1[threadIdx.x]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[2] = (conv2d_nchw_1[2] + (pad_temp.shared_1[(threadIdx.x + 16)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[4] = (conv2d_nchw_1[4] + (pad_temp.shared_1[(threadIdx.x + 32)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[6] = (conv2d_nchw_1[6] + (pad_temp.shared_1[(threadIdx.x + 48)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[8] = (conv2d_nchw_1[8] + (pad_temp.shared_1[(threadIdx.x + 64)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[10] = (conv2d_nchw_1[10] + (pad_temp.shared_1[(threadIdx.x + 80)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[12] = (conv2d_nchw_1[12] + (pad_temp.shared_1[(threadIdx.x + 96)]*placeholder.shared_1[0]))\n",
      "        conv2d_nchw_1[1] = (conv2d_nchw_1[1] + (pad_temp.shared_1[threadIdx.x]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[3] = (conv2d_nchw_1[3] + (pad_temp.shared_1[(threadIdx.x + 16)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[5] = (conv2d_nchw_1[5] + (pad_temp.shared_1[(threadIdx.x + 32)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[7] = (conv2d_nchw_1[7] + (pad_temp.shared_1[(threadIdx.x + 48)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[9] = (conv2d_nchw_1[9] + (pad_temp.shared_1[(threadIdx.x + 64)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[11] = (conv2d_nchw_1[11] + (pad_temp.shared_1[(threadIdx.x + 80)]*placeholder.shared_1[1]))\n",
      "        conv2d_nchw_1[13] = (conv2d_nchw_1[13] + (pad_temp.shared_1[(threadIdx.x + 96)]*placeholder.shared_1[1]))\n",
      "      }\n",
      "    }\n",
      "    compute_1: Buffer(compute, float32, [501760], [])[((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x)] = max(conv2d_nchw_1[0], 0f32)\n",
      "    compute_1[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 16)] = max(conv2d_nchw_1[2], 0f32)\n",
      "    compute_1[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 32)] = max(conv2d_nchw_1[4], 0f32)\n",
      "    compute_1[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 48)] = max(conv2d_nchw_1[6], 0f32)\n",
      "    compute_1[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 64)] = max(conv2d_nchw_1[8], 0f32)\n",
      "    compute_1[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 80)] = max(conv2d_nchw_1[10], 0f32)\n",
      "    compute_1[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 96)] = max(conv2d_nchw_1[12], 0f32)\n",
      "    compute_1[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 50176)] = max(conv2d_nchw_1[1], 0f32)\n",
      "    compute_1[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 50192)] = max(conv2d_nchw_1[3], 0f32)\n",
      "    compute_1[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 50208)] = max(conv2d_nchw_1[5], 0f32)\n",
      "    compute_1[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 50224)] = max(conv2d_nchw_1[7], 0f32)\n",
      "    compute_1[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 50240)] = max(conv2d_nchw_1[9], 0f32)\n",
      "    compute_1[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 50256)] = max(conv2d_nchw_1[11], 0f32)\n",
      "    compute_1[(((((blockIdx.z*100352) + (blockIdx.y*224)) + (blockIdx.x*112)) + threadIdx.x) + 50272)] = max(conv2d_nchw_1[13], 0f32)\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = te.placeholder((1, 3, 224, 224))\n",
    "kernel = te.placeholder((10, 3, 5, 5))\n",
    "\n",
    "with tvm.target.Target(\"cuda -arch=sm_86\"):\n",
    "    conv = topi.cuda.conv2d_nchw(data, kernel, 1, 2, 1)\n",
    "    out = topi.nn.relu(conv)\n",
    "    fuse = topi.cuda.schedule_conv2d_nchw([out])\n",
    "    print(tvm.lower(fuse, [data, kernel], simple_mode=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
