{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "668f0b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "import tvm.ir\n",
    "from tvm import relay\n",
    "from tvm.relay import transform\n",
    "from tvm.relay.dataflow_pattern import wildcard, is_op\n",
    "from tvm.relay.op.contrib import register_pattern_table\n",
    "from tvm.relay.testing import byoc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ebaa9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example():\n",
    "    data_dtype = \"int32\"\n",
    "    axis = 0\n",
    "    x_data = np.arange(-32, 32, 1).reshape(1, 64).astype(data_dtype)\n",
    "    y_data = np.arange(-64, 64, 2).reshape(1, 64).astype(data_dtype)\n",
    "\n",
    "    x_scale = relay.const((62 + 64) / (np.power(2, 32) - 1.0), \"float32\")\n",
    "    y_scale = relay.const((62 + 64) / (np.power(2, 32) - 1.0), \"float32\")\n",
    "    x_zero_point = relay.const(0, \"int32\")\n",
    "    y_zero_point = relay.const(0, \"int32\")\n",
    "\n",
    "    x = relay.var(\"x\", shape=(1, 64), dtype=data_dtype)\n",
    "    y = relay.var(\"y\", shape=(1, 64), dtype=data_dtype)\n",
    "    z = relay.qnn.op.concatenate(\n",
    "        (x, y),\n",
    "        input_scales=(x_scale, y_scale),\n",
    "        input_zero_points=(x_zero_point, y_zero_point),\n",
    "        output_scale=y_scale,\n",
    "        output_zero_point=relay.const(1, \"int32\"),\n",
    "        axis=axis,\n",
    "    )\n",
    "\n",
    "    func = relay.Function([x, y], z)\n",
    "    mod = tvm.IRModule.from_expr(func)\n",
    "\n",
    "mod = example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afc3f863",
   "metadata": {},
   "outputs": [
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  5: TVMFuncCall\n  4: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&)>::AssignTypedLambda<tvm::IRModule (*)(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&)>(tvm::IRModule (*)(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  3: tvm::IRModule::FromExpr(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&)\n  2: tvm::IRModule::FromExprInContext(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&, std::unordered_set<tvm::runtime::String, std::hash<tvm::runtime::String>, std::equal_to<tvm::runtime::String>, std::allocator<tvm::runtime::String> >)\n  1: tvm::IRModuleNode::Add(tvm::GlobalVar const&, tvm::BaseFunc const&, bool)\n  0: tvm::WarnIfMalformed(tvm::IRModule const&, tvm::relay::Function)\n  File \"/home/wendell/Desktop/tvm/src/ir/module.cc\", line 191\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n\n  Check failed: fv.size() == 0 (1 vs. 0) : Function:\nfn (%x: Tensor[(1, 64, 56, 56), float32]) {\n  free_var %weight: Tensor[(64, 64, 3, 3), float32];\n  %0 = add(meta[relay.Constant][0], meta[relay.Constant][0]);\n  %1 = nn.conv2d(%x, %weight, padding=[0, 0, 0, 0]);\n  %2 = multiply(%0, 2f);\n  %3 = add(%1, %2);\n  %4 = add(%3, meta[relay.Constant][0]);\n  %5 = add(%3, meta[relay.Constant][0]);\n  add(%4, %5)\n}\n\ncontains free variables: [Var(weight, ty=TensorType([64, 64, 3, 3], float32))]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mtvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIRModule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_expr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tvm/python/tvm/ir/module.py:243\u001b[0m, in \u001b[0;36mIRModule.from_expr\u001b[0;34m(expr, functions, type_defs)\u001b[0m\n\u001b[1;32m    241\u001b[0m funcs \u001b[38;5;241m=\u001b[39m functions \u001b[38;5;28;01mif\u001b[39;00m functions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    242\u001b[0m defs \u001b[38;5;241m=\u001b[39m type_defs \u001b[38;5;28;01mif\u001b[39;00m type_defs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ffi_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModule_FromExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuncs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/tvm/python/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    225\u001b[0m ret_tcode \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    227\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    236\u001b[0m ):\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    238\u001b[0m _ \u001b[38;5;241m=\u001b[39m temp_args\n\u001b[1;32m    239\u001b[0m _ \u001b[38;5;241m=\u001b[39m args\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  5: TVMFuncCall\n  4: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::IRModule (tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&)>::AssignTypedLambda<tvm::IRModule (*)(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&)>(tvm::IRModule (*)(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&), std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)\n  3: tvm::IRModule::FromExpr(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&)\n  2: tvm::IRModule::FromExprInContext(tvm::RelayExpr const&, tvm::runtime::Map<tvm::GlobalVar, tvm::BaseFunc, void, void> const&, tvm::runtime::Map<tvm::GlobalTypeVar, tvm::TypeData, void, void> const&, std::unordered_set<tvm::runtime::String, std::hash<tvm::runtime::String>, std::equal_to<tvm::runtime::String>, std::allocator<tvm::runtime::String> >)\n  1: tvm::IRModuleNode::Add(tvm::GlobalVar const&, tvm::BaseFunc const&, bool)\n  0: tvm::WarnIfMalformed(tvm::IRModule const&, tvm::relay::Function)\n  File \"/home/wendell/Desktop/tvm/src/ir/module.cc\", line 191\nTVMError: \n---------------------------------------------------------------\nAn error occurred during the execution of TVM.\nFor more information, please see: https://tvm.apache.org/docs/errors.html\n---------------------------------------------------------------\n\n  Check failed: fv.size() == 0 (1 vs. 0) : Function:\nfn (%x: Tensor[(1, 64, 56, 56), float32]) {\n  free_var %weight: Tensor[(64, 64, 3, 3), float32];\n  %0 = add(meta[relay.Constant][0], meta[relay.Constant][0]);\n  %1 = nn.conv2d(%x, %weight, padding=[0, 0, 0, 0]);\n  %2 = multiply(%0, 2f);\n  %3 = add(%1, %2);\n  %4 = add(%3, meta[relay.Constant][0]);\n  %5 = add(%3, meta[relay.Constant][0]);\n  add(%4, %5)\n}\n\ncontains free variables: [Var(weight, ty=TensorType([64, 64, 3, 3], float32))]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "430aa806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _register_external_op_helper(op_name, supported=True):\n",
    "    @tvm.ir.register_op_attr(op_name, \"target.cuda\")\n",
    "    def _func_wrapper(attrs, args):\n",
    "        return supported\n",
    "    return _func_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88847329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__._register_external_op_helper.<locals>._func_wrapper(attrs, args)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_register_external_op_helper(\"nn.conv2d\")\n",
    "_register_external_op_helper(\"nn.relu\")\n",
    "_register_external_op_helper(\"add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd04fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pattern(with_bias=True):\n",
    "    data = wildcard()\n",
    "    weight = wildcard()\n",
    "    bias = wildcard()\n",
    "    conv = is_op('nn.conv2d')(data, weight)\n",
    "    if with_bias:\n",
    "        conv_out = is_op('add')(conv, bias)\n",
    "    else:\n",
    "        conv_out = conv\n",
    "    return is_op('nn.relu')(conv_out)\n",
    "\n",
    "@register_pattern_table(\"cuda\")\n",
    "def pattern_table():\n",
    "    conv2d_bias_relu_pat = (\"cuda.conv2d_bias_relu\", make_pattern(with_bias=True))\n",
    "    conv2d_relu_pat = (\"cuda.conv2d_relu\", make_pattern(with_bias=False))\n",
    "    cuda_patterns = [conv2d_bias_relu_pat, conv2d_relu_pat]\n",
    "    return cuda_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bed62911",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'function' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMergeComposite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern_table\u001b[49m\u001b[43m)\u001b[49m(mod)\n\u001b[1;32m      2\u001b[0m mod \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mAnnotateTarget([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m])(mod) \u001b[38;5;66;03m# Output: Figure 2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m mod \u001b[38;5;241m=\u001b[39m transform\u001b[38;5;241m.\u001b[39mMergeCompilerRegions()(mod) \u001b[38;5;66;03m# Output: Figure 3\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/tvm/python/tvm/relay/transform/transform.py:556\u001b[0m, in \u001b[0;36mMergeComposite\u001b[0;34m(pattern_table)\u001b[0m\n\u001b[1;32m    554\u001b[0m patterns \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    555\u001b[0m checks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 556\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tup \u001b[38;5;129;01min\u001b[39;00m pattern_table:\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tup) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    558\u001b[0m         pattern_name, pattern \u001b[38;5;241m=\u001b[39m tup\n",
      "\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"
     ]
    }
   ],
   "source": [
    "mod = transform.MergeComposite(pattern_table)(mod)\n",
    "mod = transform.AnnotateTarget([\"cuda\"])(mod) # Output: Figure 2\n",
    "mod = transform.MergeCompilerRegions()(mod) # Output: Figure 3\n",
    "mod = transform.PartitionGraph()(mod) # Output: Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e505781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
