{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ce2210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "import numpy as np\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3908872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (M, K) X (K, N)\n",
    "M = K = N = 2**10\n",
    "dtype = 'float32'\n",
    "target = tvm.target.Target(target='llvm -mcpu=alderlake', host='llvm -mcpu=goldmont')\n",
    "dev = tvm.device(target.kind.name, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "effb199c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy running time: 0.003225\n"
     ]
    }
   ],
   "source": [
    "a = tvm.nd.array(np.random.rand(M, K).astype(dtype), dev)\n",
    "b = tvm.nd.array(np.random.rand(K, N).astype(dtype), dev)\n",
    "np_repeat = 100\n",
    "np_running_time = timeit.timeit(\n",
    "    setup='import numpy as np\\n'\n",
    "    'M=K=N=2**10\\n'\n",
    "    'dtype=\"float32\"\\n'\n",
    "    'a=np.random.rand(M, K).astype(dtype)\\n'\n",
    "    'b=np.random.rand(K, N).astype(dtype)\\n',\n",
    "    stmt='answer = np.dot(a, b)',\n",
    "    number=np_repeat\n",
    ")\n",
    "\n",
    "print('Numpy running time: %f' % (np_running_time / np_repeat))\n",
    "\n",
    "answer = np.dot(a.numpy(), b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972677d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TVM Matmul w/ TE\n",
    "from tvm import te\n",
    "import tvm.testing\n",
    "k = te.reduce_axis((0, K), 'k')\n",
    "A = te.placeholder((M, K), dtype, 'A')\n",
    "B = te.placeholder((K, N), dtype, 'B')\n",
    "C = te.compute((M, N), lambda x, y: te.sum(A[x, k] * B[k, y], axis=k), name='C')\n",
    "\n",
    "s = te.create_schedule(C.op)\n",
    "func = tvm.build(s, [A, B, C], target=target, name='matmul')\n",
    "\n",
    "c = tvm.nd.array(np.zeros((M, N), dtype=dtype), dev)\n",
    "func(a, b, c)\n",
    "\n",
    "tvm.testing.assert_allclose(c.numpy(), answer, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80ee3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_op(s, vars, tgt, name, opt, log) -> None:\n",
    "    func = tvm.build(s, vars, target=tgt, name=name)\n",
    "    assert func\n",
    "    \n",
    "    dev = tvm.device(tgt.kind.name, 0)\n",
    "    c = tvm.nd.array(np.zeros((M, N), dtype), dev)\n",
    "    func(a, b, c)\n",
    "    tvm.testing.assert_allclose(c.numpy(), answer, rtol=1e-5)\n",
    "    \n",
    "    evalor = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "    mean_time = evalor(a, b, c).mean\n",
    "    print('%s: %f' % (opt, mean_time))\n",
    "    log.append((opt, mean_time))\n",
    "    \n",
    "log = []\n",
    "# eval_op(s, [A, B, C], target,'matmul', 'none', log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c4332e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1048576], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [1048576], []),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [1048576], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1024, 1024], []), B_1: B_3: Buffer(B_2, float32, [1024, 1024], []), C_1: C_3: Buffer(C_2, float32, [1024, 1024], [])} {\n",
      "  for (x: int32, 0, 1024) {\n",
      "    for (y: int32, 0, 1024) {\n",
      "      C[((x*1024) + y)] = 0f32\n",
      "      for (k: int32, 0, 1024) {\n",
      "        let cse_var_2: int32 = (x*1024)\n",
      "        let cse_var_1: int32 = (cse_var_2 + y)\n",
      "        C[cse_var_1] = (C[cse_var_1] + (A[(cse_var_2 + k)]*B[((k*1024) + y)]))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# low level code, C here, unopt. w/ 4 loops\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7809dfad",
   "metadata": {},
   "source": [
    "## Opt 1 : Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b919434",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = 256\n",
    "xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n",
    "(k, ) = s[C].op.reduce_axis\n",
    "ko, ki = s[C].split(k, factor=4)\n",
    "s[C].reorder(xo, yo, ko, ki, xi, yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a4762ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blking: 0.115955\n"
     ]
    }
   ],
   "source": [
    "eval_op(s, [A, B, C], target,'matmul', 'blking', log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "428ee26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1048576], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [1048576], []),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [1048576], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1024, 1024], []), B_1: B_3: Buffer(B_2, float32, [1024, 1024], []), C_1: C_3: Buffer(C_2, float32, [1024, 1024], [])} {\n",
      "  for (x.outer: int32, 0, 4) {\n",
      "    for (y.outer: int32, 0, 4) {\n",
      "      for (x.inner.init: int32, 0, 256) {\n",
      "        for (y.inner.init: int32, 0, 256) {\n",
      "          C[((((x.outer*262144) + (x.inner.init*1024)) + (y.outer*256)) + y.inner.init)] = 0f32\n",
      "        }\n",
      "      }\n",
      "      for (k.outer: int32, 0, 256) {\n",
      "        for (k.inner: int32, 0, 4) {\n",
      "          for (x.inner: int32, 0, 256) {\n",
      "            for (y.inner: int32, 0, 256) {\n",
      "              let cse_var_3: int32 = (y.outer*256)\n",
      "              let cse_var_2: int32 = ((x.outer*262144) + (x.inner*1024))\n",
      "              let cse_var_1: int32 = ((cse_var_2 + cse_var_3) + y.inner)\n",
      "              C[cse_var_1] = (C[cse_var_1] + (A[((cse_var_2 + (k.outer*4)) + k.inner)]*B[((((k.outer*4096) + (k.inner*1024)) + cse_var_3) + y.inner)]))\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# more loops for mem loc.\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeec877",
   "metadata": {},
   "source": [
    "## Opt 2 : Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d0f1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only favored when 1st SIMD by passing multi data to mem.\n",
    "s[C].vectorize(yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a83b4062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorize: 0.134336\n"
     ]
    }
   ],
   "source": [
    "eval_op(s, [A, B, C], target,'matmul', 'vectorize', log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e82c89a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1048576], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [1048576], []),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [1048576], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1024, 1024], []), B_1: B_3: Buffer(B_2, float32, [1024, 1024], []), C_1: C_3: Buffer(C_2, float32, [1024, 1024], [])} {\n",
      "  for (x.outer: int32, 0, 4) {\n",
      "    for (y.outer: int32, 0, 4) {\n",
      "      for (x.inner.init: int32, 0, 256) {\n",
      "        C[ramp((((x.outer*262144) + (x.inner.init*1024)) + (y.outer*256)), 1, 256)] = broadcast(0f32, 256)\n",
      "      }\n",
      "      for (k.outer: int32, 0, 256) {\n",
      "        for (k.inner: int32, 0, 4) {\n",
      "          for (x.inner: int32, 0, 256) {\n",
      "            let cse_var_3: int32 = (y.outer*256)\n",
      "            let cse_var_2: int32 = ((x.outer*262144) + (x.inner*1024))\n",
      "            let cse_var_1: int32 = (cse_var_2 + cse_var_3)\n",
      "            C[ramp(cse_var_1, 1, 256)] = (C[ramp(cse_var_1, 1, 256)] + (broadcast(A[((cse_var_2 + (k.outer*4)) + k.inner)], 256)*B[ramp((((k.outer*4096) + (k.inner*1024)) + cse_var_3), 1, 256)]))\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92edeaac",
   "metadata": {},
   "source": [
    "## Opt 3 : Loop Permu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a06adb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop permu: 0.073387\n"
     ]
    }
   ],
   "source": [
    "s = te.create_schedule(C.op)\n",
    "xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n",
    "(k, ) = s[C].op.reduce_axis\n",
    "ko, ki = s[C].split(k, factor=4)\n",
    "\n",
    "# re-ordering, only diff. here by changing permu.\n",
    "s[C].reorder(xo, yo, ko, xi, ki, yi)\n",
    "s[C].vectorize(yi)\n",
    "\n",
    "eval_op(s, [A, B, C], target,'matmul', 'Loop permu', log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbba2e2",
   "metadata": {},
   "source": [
    "## Opt 4 : Array Packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d391572d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  12: TVMFuncCall\n  11: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)\n  10: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n  9: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n  8: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n  7: tvm::transform::Pass::operator()(tvm::IRModule) const\n  6: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  5: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  4: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  3: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  2: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_8IRModuleES5_NS_9transform11PassContextEEE17AssignTypedLambdaIZNS_3tir9transform13MakePackedAPIEiEUlS5_S7_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SF_SJ_\n  1: tvm::tir::transform::MakePackedAPI(int)::{lambda(tvm::IRModule, tvm::transform::PassContext)#1}::operator()(tvm::IRModule, tvm::transform::PassContext) const [clone .isra.0]\n  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)\n  File \"/home/wendell/Desktop/tvm/src/tir/transforms/make_packed_api.cc\", line 329\nTVMError: Not all Vars are passed in api_args:  'k'  is not bound to any variables",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m s[packedB]\u001b[38;5;241m.\u001b[39mvectorize(z)\n\u001b[1;32m     17\u001b[0m s[packedB]\u001b[38;5;241m.\u001b[39mparallel(x)\n\u001b[0;32m---> 19\u001b[0m \u001b[43meval_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmatmul\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mArray packing\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(tvm\u001b[38;5;241m.\u001b[39mlower(s, [A, B, C], simple_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36meval_op\u001b[0;34m(s, vars, tgt, name, opt, log)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_op\u001b[39m(s, \u001b[38;5;28mvars\u001b[39m, tgt, name, opt, log) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 2\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[43mtvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m func\n\u001b[1;32m      5\u001b[0m     dev \u001b[38;5;241m=\u001b[39m tvm\u001b[38;5;241m.\u001b[39mdevice(tgt\u001b[38;5;241m.\u001b[39mkind\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/tvm/python/tvm/driver/build_module.py:284\u001b[0m, in \u001b[0;36mbuild\u001b[0;34m(inputs, args, target, target_host, runtime, name, binds)\u001b[0m\n\u001b[1;32m    280\u001b[0m     target_host \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllvm\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tvm\u001b[38;5;241m.\u001b[39mruntime\u001b[38;5;241m.\u001b[39menabled(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllvm\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstackvm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m annotated_mods, target_host \u001b[38;5;241m=\u001b[39m Target\u001b[38;5;241m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[0;32m--> 284\u001b[0m rt_mod_host \u001b[38;5;241m=\u001b[39m \u001b[43m_driver_ffi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtir_to_runtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotated_mods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_host\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m annotated_mods, target_host \u001b[38;5;241m=\u001b[39m Target\u001b[38;5;241m.\u001b[39mcanon_target_map_and_host(annotated_mods, target_host)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target_host, Target):\n",
      "File \u001b[0;32m~/Desktop/tvm/python/tvm/_ffi/_ctypes/packed_func.py:237\u001b[0m, in \u001b[0;36mPackedFuncBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    225\u001b[0m ret_tcode \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_int()\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    227\u001b[0m     _LIB\u001b[38;5;241m.\u001b[39mTVMFuncCall(\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    236\u001b[0m ):\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m get_last_ffi_error()\n\u001b[1;32m    238\u001b[0m _ \u001b[38;5;241m=\u001b[39m temp_args\n\u001b[1;32m    239\u001b[0m _ \u001b[38;5;241m=\u001b[39m args\n",
      "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  12: TVMFuncCall\n  11: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::TypedPackedFunc<tvm::runtime::Module (tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)>::AssignTypedLambda<tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}>(tvm::{lambda(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target)#6}, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)::{lambda(tvm::runtime::TVMArgs const&, tvm::runtime::TVMRetValue*)#1}> >::Call(tvm::runtime::PackedFuncObj const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tvm::runtime::TVMRetValue)\n  10: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n  9: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n  8: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n  7: tvm::transform::Pass::operator()(tvm::IRModule) const\n  6: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  5: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  4: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  3: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  2: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_8IRModuleES5_NS_9transform11PassContextEEE17AssignTypedLambdaIZNS_3tir9transform13MakePackedAPIEiEUlS5_S7_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SF_SJ_\n  1: tvm::tir::transform::MakePackedAPI(int)::{lambda(tvm::IRModule, tvm::transform::PassContext)#1}::operator()(tvm::IRModule, tvm::transform::PassContext) const [clone .isra.0]\n  0: tvm::tir::MakePackedAPI(tvm::tir::PrimFunc&&, int)\n  File \"/home/wendell/Desktop/tvm/src/tir/transforms/make_packed_api.cc\", line 329\nTVMError: Not all Vars are passed in api_args:  'k'  is not bound to any variables"
     ]
    }
   ],
   "source": [
    "packedB = te.compute((N / bn, K, bn), lambda x, y, z: B[y, x * bn + z], name='packedB')\n",
    "C = te.compute(\n",
    "    (M, N),\n",
    "    lambda x, y: te.sum(A[x, k] * packedB[y // bn, k, tvm.tir.indexmod(y, bn)], axis=k),\n",
    "    name='C'\n",
    ")\n",
    "\n",
    "s = te.create_schedule(C.op)\n",
    "xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n",
    "(k, ) = s[C].op.reduce_axis\n",
    "ko, ki = s[C].split(k, factor=4)\n",
    "s[C].reorder(xo, yo, ko, xi, ki, yi)\n",
    "s[C].vectorize(yi)\n",
    "\n",
    "x, y, z = s[packedB].op.axis\n",
    "s[packedB].vectorize(z)\n",
    "s[packedB].parallel(x)\n",
    "\n",
    "eval_op(s, [A, B, C], target,'matmul', 'Array packing', log)\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36ae737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
