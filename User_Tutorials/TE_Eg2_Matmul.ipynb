{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46072448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "import numpy as np\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4053527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (M, K) X (K, N)\n",
    "M = K = N = 2**10\n",
    "dtype = 'float32'\n",
    "target = tvm.target.Target(target='llvm -mcpu=alderlake', host='llvm -mcpu=goldmont')\n",
    "dev = tvm.device(target.kind.name, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a2b37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy running time: 0.002995\n"
     ]
    }
   ],
   "source": [
    "a = tvm.nd.array(np.random.rand(M, K).astype(dtype), dev)\n",
    "b = tvm.nd.array(np.random.rand(K, N).astype(dtype), dev)\n",
    "np_repeat = 100\n",
    "np_running_time = timeit.timeit(\n",
    "    setup='import numpy as np\\n'\n",
    "    'M=K=N=2**10\\n'\n",
    "    'dtype=\"float32\"\\n'\n",
    "    'a=np.random.rand(M, K).astype(dtype)\\n'\n",
    "    'b=np.random.rand(K, N).astype(dtype)\\n',\n",
    "    stmt='answer = np.dot(a, b)',\n",
    "    number=np_repeat\n",
    ")\n",
    "\n",
    "print('Numpy running time: %f' % (np_running_time / np_repeat))\n",
    "\n",
    "answer = np.dot(a.numpy(), b.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da7aa9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TVM Matmul w/ TE\n",
    "from tvm import te\n",
    "import tvm.testing\n",
    "k = te.reduce_axis((0, K), 'k')\n",
    "A = te.placeholder((M, K), dtype, 'A')\n",
    "B = te.placeholder((K, N), dtype, 'B')\n",
    "C = te.compute((M, N), lambda x, y: te.sum(A[x, k] * B[k, y], axis=k), name='C')\n",
    "\n",
    "s = te.create_schedule(C.op)\n",
    "func = tvm.build(s, [A, B, C], target=target, name='matmul')\n",
    "\n",
    "c = tvm.nd.array(np.zeros((M, N), dtype=dtype), dev)\n",
    "func(a, b, c)\n",
    "\n",
    "tvm.testing.assert_allclose(c.numpy(), answer, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8c0f90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none: 2.010295\n"
     ]
    }
   ],
   "source": [
    "def eval_op(s, vars, tgt, name, opt, log) -> None:\n",
    "    func = tvm.build(s, vars, target=tgt, name=name)\n",
    "    assert func\n",
    "    \n",
    "    dev = tvm.device(tgt.kind.name, 0)\n",
    "    c = tvm.nd.array(np.zeros((M, N), dtype), dev)\n",
    "    func(a, b, c)\n",
    "    tvm.testing.assert_allclose(c.numpy(), answer, rtol=1e-5)\n",
    "    \n",
    "    evalor = func.time_evaluator(func.entry_name, dev, number=1)\n",
    "    mean_time = evalor(a, b, c).mean\n",
    "    print('%s: %f' % (opt, mean_time))\n",
    "    log.append((opt, mean_time))\n",
    "    \n",
    "log = []\n",
    "eval_op(s, [A, B, C], target,'matmul', 'none', log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d45bcf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1048576], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [1048576], []),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [1048576], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1024, 1024], []), B_1: B_3: Buffer(B_2, float32, [1024, 1024], []), C_1: C_3: Buffer(C_2, float32, [1024, 1024], [])} {\n",
      "  for (x: int32, 0, 1024) {\n",
      "    for (y: int32, 0, 1024) {\n",
      "      C[((x*1024) + y)] = 0f32\n",
      "      for (k: int32, 0, 1024) {\n",
      "        let cse_var_2: int32 = (x*1024)\n",
      "        let cse_var_1: int32 = (cse_var_2 + y)\n",
      "        C[cse_var_1] = (C[cse_var_1] + (A[(cse_var_2 + k)]*B[((k*1024) + y)]))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# low level code, C here, unopt. w/ 4 loops\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b9bcb",
   "metadata": {},
   "source": [
    "## Opt 1 : Blocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b58e5502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opt 1 : Blocking\n",
    "bn = 32\n",
    "xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n",
    "(k,) = s[C].op.reduce_axis\n",
    "ko, ki = s[C].split(k, factor=4)\n",
    "\n",
    "s[C].reorder(xo, yo, ko, ki, xi, yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccdcd690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blking: 0.135766\n"
     ]
    }
   ],
   "source": [
    "eval_op(s, [A, B, C], target,'matmul', 'blking', log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f2de01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1048576], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [1048576], []),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [1048576], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1024, 1024], []), B_1: B_3: Buffer(B_2, float32, [1024, 1024], []), C_1: C_3: Buffer(C_2, float32, [1024, 1024], [])} {\n",
      "  for (x.outer: int32, 0, 32) {\n",
      "    for (y.outer: int32, 0, 32) {\n",
      "      for (x.inner.init: int32, 0, 32) {\n",
      "        for (y.inner.init: int32, 0, 32) {\n",
      "          C[((((x.outer*32768) + (x.inner.init*1024)) + (y.outer*32)) + y.inner.init)] = 0f32\n",
      "        }\n",
      "      }\n",
      "      for (k.outer: int32, 0, 256) {\n",
      "        for (k.inner: int32, 0, 4) {\n",
      "          for (x.inner: int32, 0, 32) {\n",
      "            for (y.inner: int32, 0, 32) {\n",
      "              let cse_var_3: int32 = (y.outer*32)\n",
      "              let cse_var_2: int32 = ((x.outer*32768) + (x.inner*1024))\n",
      "              let cse_var_1: int32 = ((cse_var_2 + cse_var_3) + y.inner)\n",
      "              C[cse_var_1] = (C[cse_var_1] + (A[((cse_var_2 + (k.outer*4)) + k.inner)]*B[((((k.outer*4096) + (k.inner*1024)) + cse_var_3) + y.inner)]))\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# more loops for mem loc.\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0fe166",
   "metadata": {},
   "source": [
    "## Opt 2 : Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01ee077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only favored when 1st SIMD by passing multi data to mem.\n",
    "def vecotorize(s, vars):\n",
    "    s = blocking(vars)\n",
    "    (A, B, C) = vars\n",
    "    s[C].vectorize(yi)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e67949a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorize: 0.137350\n"
     ]
    }
   ],
   "source": [
    "eval_op(s, [A, B, C], target,'matmul', 'vectorize', log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40094eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1048576], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [1048576], []),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [1048576], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1024, 1024], []), B_1: B_3: Buffer(B_2, float32, [1024, 1024], []), C_1: C_3: Buffer(C_2, float32, [1024, 1024], [])} {\n",
      "  for (x.outer: int32, 0, 32) {\n",
      "    for (y.outer: int32, 0, 32) {\n",
      "      for (x.inner.init: int32, 0, 32) {\n",
      "        for (y.inner.init: int32, 0, 32) {\n",
      "          C[((((x.outer*32768) + (x.inner.init*1024)) + (y.outer*32)) + y.inner.init)] = 0f32\n",
      "        }\n",
      "      }\n",
      "      for (k.outer: int32, 0, 256) {\n",
      "        for (k.inner: int32, 0, 4) {\n",
      "          for (x.inner: int32, 0, 32) {\n",
      "            for (y.inner: int32, 0, 32) {\n",
      "              let cse_var_3: int32 = (y.outer*32)\n",
      "              let cse_var_2: int32 = ((x.outer*32768) + (x.inner*1024))\n",
      "              let cse_var_1: int32 = ((cse_var_2 + cse_var_3) + y.inner)\n",
      "              C[cse_var_1] = (C[cse_var_1] + (A[((cse_var_2 + (k.outer*4)) + k.inner)]*B[((((k.outer*4096) + (k.inner*1024)) + cse_var_3) + y.inner)]))\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0f49b",
   "metadata": {},
   "source": [
    "## Opt 3 : Loop Permu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c28c8dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop permu: 0.048924\n"
     ]
    }
   ],
   "source": [
    "s = te.create_schedule(C.op)\n",
    "xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n",
    "(k, ) = s[C].op.reduce_axis\n",
    "ko, ki = s[C].split(k, factor=4)\n",
    "\n",
    "# re-ordering, only diff. here by changing permu.\n",
    "s[C].reorder(xo, yo, ko, xi, ki, yi)\n",
    "s[C].vectorize(yi)\n",
    "\n",
    "eval_op(s, [A, B, C], target,'matmul', 'Loop permu', log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1976f",
   "metadata": {},
   "source": [
    "## Opt 4 : Array Packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76a82d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array packing: 0.054949\n",
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1048576], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [1048576], []),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [1048576], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1024, 1024], []), B_1: B_3: Buffer(B_2, float32, [1024, 1024], []), C_1: C_3: Buffer(C_2, float32, [1024, 1024], [])} {\n",
      "  allocate(packedB: Pointer(global float32x32), float32x32, [32768]), storage_scope = global {\n",
      "    for (x: int32, 0, 32) \"parallel\" {\n",
      "      for (y: int32, 0, 1024) {\n",
      "        packedB_1: Buffer(packedB, float32x32, [32768], [])[((x*1024) + y)] = B[ramp(((y*1024) + (x*32)), 1, 32)]\n",
      "      }\n",
      "    }\n",
      "    for (x.outer: int32, 0, 32) {\n",
      "      for (y.outer: int32, 0, 32) {\n",
      "        for (x.inner.init: int32, 0, 32) {\n",
      "          C[ramp((((x.outer*32768) + (x.inner.init*1024)) + (y.outer*32)), 1, 32)] = broadcast(0f32, 32)\n",
      "        }\n",
      "        for (k.outer: int32, 0, 256) {\n",
      "          for (x.inner: int32, 0, 32) {\n",
      "            for (k.inner: int32, 0, 4) {\n",
      "              let cse_var_3: int32 = ((x.outer*32768) + (x.inner*1024))\n",
      "              let cse_var_2: int32 = (k.outer*4)\n",
      "              let cse_var_1: int32 = (cse_var_3 + (y.outer*32))\n",
      "              C[ramp(cse_var_1, 1, 32)] = (C[ramp(cse_var_1, 1, 32)] + (broadcast(A[((cse_var_3 + cse_var_2) + k.inner)], 32)*packedB_1[(((y.outer*1024) + cse_var_2) + k.inner)]))\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[10:38:54] /home/wendell/Desktop/tvm/src/runtime/threading_backend.cc:320: Warning: more than two frequencies detected!\n"
     ]
    }
   ],
   "source": [
    "packedB = te.compute((N / bn, K, bn), lambda x, y, z: B[y, x * bn + z], name='packedB')\n",
    "C = te.compute(\n",
    "    (M, N),\n",
    "    lambda x, y: te.sum(A[x, k] * packedB[y // bn, k, tvm.tir.indexmod(y, bn)], axis=k),\n",
    "    name='C'\n",
    ")\n",
    "\n",
    "s = te.create_schedule(C.op)\n",
    "xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n",
    "(k, ) = s[C].op.reduce_axis\n",
    "ko, ki = s[C].split(k, factor=4)\n",
    "s[C].reorder(xo, yo, ko, xi, ki, yi)\n",
    "s[C].vectorize(yi)\n",
    "\n",
    "x, y, z = s[packedB].op.axis\n",
    "s[packedB].vectorize(z)\n",
    "s[packedB].parallel(x)\n",
    "\n",
    "eval_op(s, [A, B, C], target,'matmul', 'Array packing', log)\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c178d916",
   "metadata": {},
   "source": [
    "## Opt 5 : Write Thru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdd0d9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write Thru: 0.052422\n",
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1048576], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [1048576], []),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [1048576], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1024, 1024], []), B_1: B_3: Buffer(B_2, float32, [1024, 1024], []), C_1: C_3: Buffer(C_2, float32, [1024, 1024], [])} {\n",
      "  allocate(packedB: Pointer(global float32x32), float32x32, [32768]), storage_scope = global;\n",
      "  allocate(C.global: Pointer(global float32), float32, [1024]), storage_scope = global {\n",
      "    for (x: int32, 0, 32) \"parallel\" {\n",
      "      for (y: int32, 0, 1024) {\n",
      "        packedB_1: Buffer(packedB, float32x32, [32768], [])[((x*1024) + y)] = B[ramp(((y*1024) + (x*32)), 1, 32)]\n",
      "      }\n",
      "    }\n",
      "    for (x.outer: int32, 0, 32) {\n",
      "      for (y.outer: int32, 0, 32) {\n",
      "        for (x.c.init: int32, 0, 32) {\n",
      "          C.global_1: Buffer(C.global, float32, [1024], [])[ramp((x.c.init*32), 1, 32)] = broadcast(0f32, 32)\n",
      "        }\n",
      "        for (k.outer: int32, 0, 256) {\n",
      "          for (x.c: int32, 0, 32) {\n",
      "            let cse_var_4: int32 = (k.outer*4)\n",
      "            let cse_var_3: int32 = (x.c*32)\n",
      "            let cse_var_2: int32 = ((y.outer*1024) + cse_var_4)\n",
      "            let cse_var_1: int32 = (((x.outer*32768) + (x.c*1024)) + cse_var_4)\n",
      "             {\n",
      "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A[cse_var_1], 32)*packedB_1[cse_var_2]))\n",
      "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A[(cse_var_1 + 1)], 32)*packedB_1[(cse_var_2 + 1)]))\n",
      "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A[(cse_var_1 + 2)], 32)*packedB_1[(cse_var_2 + 2)]))\n",
      "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A[(cse_var_1 + 3)], 32)*packedB_1[(cse_var_2 + 3)]))\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        for (x.inner: int32, 0, 32) {\n",
      "          for (y.inner: int32, 0, 32) {\n",
      "            C[((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)) + y.inner)] = C.global_1[((x.inner*32) + y.inner)]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = te.create_schedule(C.op)\n",
    "\n",
    "# Allocate write cache\n",
    "CC = s.cache_write(C, \"global\")\n",
    "\n",
    "xo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n",
    "\n",
    "# Write cache is computed at yo\n",
    "s[CC].compute_at(s[C], yo)\n",
    "\n",
    "# New inner axes\n",
    "xc, yc = s[CC].op.axis\n",
    "\n",
    "(k,) = s[CC].op.reduce_axis\n",
    "ko, ki = s[CC].split(k, factor=4)\n",
    "s[CC].reorder(ko, xc, ki, yc)\n",
    "s[CC].unroll(ki)\n",
    "s[CC].vectorize(yc)\n",
    "\n",
    "x, y, z = s[packedB].op.axis\n",
    "s[packedB].vectorize(z)\n",
    "s[packedB].parallel(x)\n",
    "\n",
    "eval_op(s, [A, B, C], target,'matmul', 'Write Thru', log)\n",
    "\n",
    "# Here is the generated IR after write cache blocking.\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81e3ccb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel: 0.020728\n",
      "@main = primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [1048576], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [1048576], []),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [1048576], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1024, 1024], []), B_1: B_3: Buffer(B_2, float32, [1024, 1024], []), C_1: C_3: Buffer(C_2, float32, [1024, 1024], [])} {\n",
      "  allocate(packedB: Pointer(global float32x32), float32x32, [32768]), storage_scope = global {\n",
      "    for (x: int32, 0, 32) \"parallel\" {\n",
      "      for (y: int32, 0, 1024) {\n",
      "        packedB_1: Buffer(packedB, float32x32, [32768], [])[((x*1024) + y)] = B[ramp(((y*1024) + (x*32)), 1, 32)]\n",
      "      }\n",
      "    }\n",
      "    for (x.outer: int32, 0, 32) \"parallel\" {\n",
      "      allocate(C.global: Pointer(global float32), float32, [1024]), storage_scope = global;\n",
      "      for (y.outer: int32, 0, 32) {\n",
      "        for (x.c.init: int32, 0, 32) {\n",
      "          C.global_1: Buffer(C.global, float32, [1024], [])[ramp((x.c.init*32), 1, 32)] = broadcast(0f32, 32)\n",
      "        }\n",
      "        for (k.outer: int32, 0, 256) {\n",
      "          for (x.c: int32, 0, 32) {\n",
      "            let cse_var_4: int32 = (k.outer*4)\n",
      "            let cse_var_3: int32 = (x.c*32)\n",
      "            let cse_var_2: int32 = ((y.outer*1024) + cse_var_4)\n",
      "            let cse_var_1: int32 = (((x.outer*32768) + (x.c*1024)) + cse_var_4)\n",
      "             {\n",
      "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A[cse_var_1], 32)*packedB_1[cse_var_2]))\n",
      "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A[(cse_var_1 + 1)], 32)*packedB_1[(cse_var_2 + 1)]))\n",
      "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A[(cse_var_1 + 2)], 32)*packedB_1[(cse_var_2 + 2)]))\n",
      "              C.global_1[ramp(cse_var_3, 1, 32)] = (C.global_1[ramp(cse_var_3, 1, 32)] + (broadcast(A[(cse_var_1 + 3)], 32)*packedB_1[(cse_var_2 + 3)]))\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        for (x.inner: int32, 0, 32) {\n",
      "          for (y.inner: int32, 0, 32) {\n",
      "            C[((((x.outer*32768) + (x.inner*1024)) + (y.outer*32)) + y.inner)] = C.global_1[((x.inner*32) + y.inner)]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# parallel\n",
    "s[C].parallel(xo)\n",
    "\n",
    "x, y, z = s[packedB].op.axis\n",
    "s[packedB].vectorize(z)\n",
    "s[packedB].parallel(x)\n",
    "\n",
    "eval_op(s, [A, B, C], target,'matmul', 'Parallel', log)\n",
    "\n",
    "# Here is the generated IR after parallelization.\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4fe35ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('none', 2.010295384),\n",
       " ('blking', 0.135766278),\n",
       " ('vectorize', 0.137349809),\n",
       " ('Loop permu', 0.048923863),\n",
       " ('Array packing', 0.054949382),\n",
       " ('Write Thru', 0.052422102000000005),\n",
       " ('Parallel', 0.020728181)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
